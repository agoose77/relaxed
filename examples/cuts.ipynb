{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax.random import PRNGKey\n",
    "import matplotlib.pyplot as plt\n",
    "import relaxed\n",
    "from functools import partial\n",
    "\n",
    "# matplotlib settings\n",
    "plt.rc(\"figure\", figsize=(6, 3), dpi=150, facecolor=\"w\")\n",
    "plt.rc(\"legend\", fontsize=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimising cuts using gradient descent\n",
    "\n",
    "This is a simple example to show how you would implement optimisation of cuts for best significance in a differentiable way, and examines some of the trade-offs that one has to make in order to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate background data from an exponential distribution with a little noise\n",
    "def generate_background(key, n_samples, n_features, noise_std):\n",
    "    key, subkey = jax.random.split(key, 2)\n",
    "    data = jax.random.exponential(subkey, (n_samples, n_features))\n",
    "    key, subkey = jax.random.split(key, 2)\n",
    "    data += jax.random.normal(subkey, (n_samples, n_features)) * noise_std\n",
    "    return data\n",
    "\n",
    "\n",
    "# generate signal data from a normal distribution close to the background\n",
    "def generate_signal(key, n_samples, n_features):\n",
    "    key, subkey = jax.random.split(key, 2)\n",
    "    data = jax.random.normal(subkey, (n_samples, n_features)) / 2 + 2\n",
    "    return data\n",
    "\n",
    "\n",
    "# get 1000 samples from the background and 100 samples from the signal\n",
    "bkg = generate_background(PRNGKey(0), 1000, 1, 0.1).ravel()\n",
    "sig = generate_signal(PRNGKey(1), 100, 1).ravel()\n",
    "\n",
    "# plot!\n",
    "plt.hist(\n",
    "    [bkg, sig], stacked=True, bins=30, histtype=\"step\", label=[\"background\", \"signal\"]\n",
    ")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"count\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's treat this as a one-bin counting experiment, where we'll make a cut (=keep data above/below a threshold value) in order to get a better chance of discovering the signal.\n",
    "\n",
    "To do this, let's write a function to get the significance, and get its value at a random cut value (that you can change!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def significance_after_cut(cut):\n",
    "    # treat analysis as a one-bin counting experiment\n",
    "    s = len(sig[sig > cut])\n",
    "    b = len(bkg[bkg > cut])\n",
    "    return relaxed.metrics.asimov_sig(s, b)  # stat-only significance\n",
    "\n",
    "\n",
    "cut = 2  # change me to change the plot!\n",
    "\n",
    "significance = significance_after_cut(cut)\n",
    "plt.hist(\n",
    "    [bkg, sig], stacked=True, bins=30, histtype=\"step\", label=[\"background\", \"signal\"]\n",
    ")\n",
    "plt.axvline(x=cut, color=\"k\", linestyle=\"--\", alpha=0.5, label=f\"cut = {cut:.2f}\")\n",
    "plt.text(\n",
    "    0.5,\n",
    "    0.5,\n",
    "    f\"significance at cut = {significance:.2f}\",\n",
    "    ha=\"center\",\n",
    "    va=\"center\",\n",
    "    transform=plt.gca().transAxes,\n",
    ")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"count\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's the best cut we can make? We can find out through a quick grid search over all cut values since this is a toy example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot significance for all cut values\n",
    "cut_values = jnp.linspace(0, 8, 100)\n",
    "significances_hard = jnp.array([significance_after_cut(cut) for cut in cut_values])\n",
    "plt.plot(cut_values, significances_hard, label=\"significance\")\n",
    "optimal_cut = cut_values[jnp.argmax(significances_hard)]\n",
    "plt.axvline(x=optimal_cut, color=\"k\", linestyle=\"--\", alpha=0.5, label=\"optimal cut\")\n",
    "plt.text(\n",
    "    0.5,\n",
    "    0.5,\n",
    "    f\"optimal cut = {optimal_cut:.2f}\",\n",
    "    ha=\"center\",\n",
    "    va=\"center\",\n",
    "    transform=plt.gca().transAxes,\n",
    ")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"significance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll introduce `relaxed.cut`, which is a softened version of a cut that uses the sigmoid function $1/(1+e^{-x})$. Normally, the sigmoid serves to map values on the real line to [0,1], so we leverage this to be used as a cut by applying it to data, which results in a set of weights for each point in [0,1]. (A normal cut does this too, but the weights are all 0 or 1, and you drop the 0s. One could similarly threshold on a minimum weight value here.)\n",
    "\n",
    "Practically, we introduce slope and intercept terms that control the sigmoid's x position and how \"hard\" the cut is: $1/(1+e^{-\\mathrm{slope}(x-\\mathrm{cut~value}})$. High slopes mean less approximate cuts, but at the risk of gradient instability.\n",
    "\n",
    "Let's look at this in practice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut_val = 5  # translates on the x-axis\n",
    "plt.plot(cut_values, cut_values > cut_val, label=\"hard cut\")\n",
    "plt.plot(\n",
    "    cut_values, relaxed.cut(cut_values, cut_val, slope=1), label=\"soft cut\", color=\"C1\"\n",
    ")\n",
    "plt.plot(\n",
    "    cut_values,\n",
    "    relaxed.cut(cut_values, cut_val, slope=10),\n",
    "    label=\"less soft cut (high slope)\",\n",
    "    color=\"C2\",\n",
    "    alpha=0.4,\n",
    ")\n",
    "plt.plot(\n",
    "    cut_values,\n",
    "    relaxed.cut(cut_values, cut_val, slope=0.5),\n",
    "    label=\"more soft cut (low slope)\",\n",
    "    color=\"C3\",\n",
    "    alpha=0.4,\n",
    ")\n",
    "plt.ylabel(\"weight applied at x\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the cuts are approximate, the significance will also be approximate. Not to worry, as we're going to use the optimised cut value in an analysis with actual cuts (although there's nothing stopping us from using the soft cuts instead), and trust that the optimised cut value is still good. We can do this if the soft cut-significance faithfully represents the shape of the actual significance, which we can check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def significance_after_soft_cut(cut, slope):\n",
    "    s_weights = (\n",
    "        relaxed.cut(sig, cut, slope) + 1e-4\n",
    "    )  # add small offset to avoid 0 weights\n",
    "    b_weights = relaxed.cut(bkg, cut, slope) + 1e-4\n",
    "    return relaxed.metrics.asimov_sig(s_weights.sum(), b_weights.sum())\n",
    "\n",
    "\n",
    "# choosing the cut slope: increasing slope reduces bias but also noises gradients.\n",
    "# I increased it until gradients were nan in the next step, then went a touch lower.\n",
    "# I'll think about a more principled way to do this (suggestions welcome!)\n",
    "slope = 2.7\n",
    "\n",
    "# plot significance for all cut values\n",
    "cut_values = jnp.linspace(0, 8, 100)\n",
    "soft = partial(significance_after_soft_cut, slope=slope)\n",
    "significances = jax.vmap(soft, in_axes=(0))(cut_values)\n",
    "plt.plot(cut_values, significances_hard, label=\"significance with hard cut\")\n",
    "plt.plot(cut_values, significances, label=\"significance with soft cut\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"significance\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the maxima of the significance are in the same region as the softened version, so we're good to go from an optimisation standpoint!\n",
    "\n",
    "We'll use gradient descent through the adam optimiser, and see how we do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jaxopt import OptaxSolver\n",
    "from optax import adam\n",
    "\n",
    "# define something to minimise (1/significance)\n",
    "def loss(cut):\n",
    "    s_weights = relaxed.cut(sig, cut, slope) + 1e-4\n",
    "    b_weights = relaxed.cut(bkg, cut, slope) + 1e-4\n",
    "    return 1 / relaxed.metrics.asimov_sig(s_weights.sum(), b_weights.sum())\n",
    "\n",
    "\n",
    "# play with the keyword arguments to the optimiser if you want :)\n",
    "solver = OptaxSolver(loss, adam(learning_rate=1e-2), maxiter=10000, tol=1e-6)\n",
    "init = 6.0\n",
    "cut_opt = solver.run(init).params\n",
    "cut_opt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, now let's see how good we did!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "significance = significance_after_cut(cut_opt)\n",
    "plt.hist(\n",
    "    [bkg, sig], stacked=True, bins=30, histtype=\"step\", label=[\"background\", \"signal\"]\n",
    ")\n",
    "plt.axvline(\n",
    "    x=cut_opt,\n",
    "    color=\"r\",\n",
    "    linestyle=\"-\",\n",
    "    alpha=0.5,\n",
    "    label=f\"optimised cut = {cut_opt:.2f}\",\n",
    ")\n",
    "significance = significance_after_cut(cut_opt)\n",
    "plt.axvline(\n",
    "    x=optimal_cut,\n",
    "    color=\"k\",\n",
    "    linestyle=\"--\",\n",
    "    alpha=0.5,\n",
    "    label=f\"true best cut = {optimal_cut:.2f}\",\n",
    ")\n",
    "plt.text(\n",
    "    0.6,\n",
    "    0.5,\n",
    "    f\"significance at optimised cut = {significance:.2f}\",\n",
    "    ha=\"center\",\n",
    "    va=\"center\",\n",
    "    transform=plt.gca().transAxes,\n",
    ")\n",
    "plt.text(\n",
    "    0.6,\n",
    "    0.6,\n",
    "    f\"significance at true best cut = {significance_after_cut(optimal_cut):.2f}\",\n",
    "    ha=\"center\",\n",
    "    va=\"center\",\n",
    "    transform=plt.gca().transAxes,\n",
    ")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"count\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On my computer, we're .01 away from the best significance, so I'm pretty happy with that ;)\n",
    "\n",
    "Thanks go to Alex Held, who provided a very similar nice example in the early days that I used as inspiration ([see here](https://github.com/alexander-held/differentiable-analysis-example/blob/master/Significance_optimization.ipynb))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "22d6333b89854cd01c2018f3ca2f5a59a2cde2765fbca789ff36cfad48ca629b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
