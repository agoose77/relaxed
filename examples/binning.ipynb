{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax.random import PRNGKey\n",
    "import matplotlib.pyplot as plt\n",
    "import relaxed\n",
    "from jaxopt import OptaxSolver\n",
    "from optax import adam\n",
    "from functools import partial\n",
    "import jax.scipy as jsp\n",
    "from plothelp import autogrid\n",
    "import pyhf\n",
    "\n",
    "pyhf.set_backend(\"jax\")\n",
    "\n",
    "# matplotlib settings\n",
    "plt.rc(\"figure\", figsize=(6, 3), dpi=150, facecolor=\"w\")\n",
    "plt.rc(\"legend\", fontsize=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binning optimisation with `relaxed` and gradient descent\n",
    "\n",
    "Normally, when we choose a binning, we pick something with uniform widths. It's easy, convenient, and can often aid in reasoning. However, if you wanted to squeeze every last drop of sensitivity out of an analysis, this binning (i.e. the location of the bin edges) can be further optimised for the best significance. That's what we'll be exploring below.\n",
    "\n",
    "To start, we'll generate some toy signal and background data to play with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate background data from an exponential distribution with a little noise\n",
    "def generate_background(key, n_samples, n_features, noise_std):\n",
    "    key, subkey = jax.random.split(key, 2)\n",
    "    data = jax.random.exponential(subkey, (n_samples, n_features))\n",
    "    key, subkey = jax.random.split(key, 2)\n",
    "    data += jax.random.normal(subkey, (n_samples, n_features)) * noise_std\n",
    "    return data\n",
    "\n",
    "\n",
    "# generate signal data from a normal distribution close to the background\n",
    "def generate_signal(key, n_samples, n_features):\n",
    "    key, subkey = jax.random.split(key, 2)\n",
    "    data = jax.random.normal(subkey, (n_samples, n_features)) / 2 + 2\n",
    "    return data\n",
    "\n",
    "\n",
    "# get 1000 samples from the background and 100 samples from the signal\n",
    "bkg = generate_background(PRNGKey(0), 1000, 1, 0.1).ravel()\n",
    "sig = generate_signal(PRNGKey(1), 100, 1).ravel()\n",
    "\n",
    "plt.hist(\n",
    "    [bkg, sig], stacked=True, bins=30, histtype=\"step\", label=[\"background\", \"signal\"]\n",
    ")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we've just used an arbitrary binning for display purposes, but now we'll seek to find something more optimal. We can come up with a criteria for this using the infamous formula for median discovery significance in the case of known background:\n",
    "\n",
    "$$\n",
    "Z_A = 2 \\sum_{i=0}^{\\mathrm{num~bins-1}}(s_i + b_i)\\log(1 + s_i / b_i) - s_i\n",
    "$$\n",
    "\n",
    "We can then seek to minimise 1/$Z_A$ (since there are far more minimisation than maximisation routines) using gradient descent. One optimisation step will then involve constructing the histogram with our current binning, calculating the significance of that histogram, then updating the bin edges with the gradient of the significance. For the last step, we'll use the `jaxopt` library for minimisation, and the `adam` optimiser from `optax`.\n",
    "\n",
    "There is one important caveat though -- this workflow contains histograms, which are not a differentiable operation. To circumvent this issue, we'll be using `relaxed.hist`, which is a surrogate for the traditional histogram calculated via kernel density estimation. The main difference is that `relaxed.hist` has a parameter called the *bandwidth*, where lower bandwidth = less approximation [but more unstable gradients]. (see section 2.1 of https://arxiv.org/abs/2203.05570 for more info). Note that after optimisation, we'd use this binning for a true histogram, so we should calculate that significance too as our main performance measure.\n",
    "\n",
    "Below, we set this all up, then perform the optimisation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# differentiable significance pipeline\n",
    "def asimov_pipe(bins, s, b, bandwidth=1e-2):\n",
    "    # restrict bin edges so they don't overlap during optimisation\n",
    "    bins_new = jnp.concatenate(\n",
    "        (\n",
    "            jnp.array([bins[0]]),\n",
    "            jnp.where(bins[1:] > bins[:-1], bins[1:], bins[:-1] + 1e-4),\n",
    "        ),\n",
    "        axis=0,\n",
    "    )\n",
    "    # make differentiable histograms for each dataset\n",
    "    s_hist = relaxed.hist(s, bins=bins_new, bandwidth=bandwidth) + 1e-4\n",
    "    b_hist = relaxed.hist(b, bins=bins_new, bandwidth=bandwidth) + 1e-4\n",
    "    # 1/significance because we want to minimise this function\n",
    "    return 1 / relaxed.metrics.asimov_sig(s_hist, b_hist)\n",
    "\n",
    "\n",
    "# significance with true histograms for comparison\n",
    "def hard_asimov_pipe(bins, s, b):\n",
    "    s_hist = jnp.histogram(s, bins=bins)[0] + 1e-4\n",
    "    b_hist = jnp.histogram(b, bins=bins)[0] + 1e-4\n",
    "    return relaxed.metrics.asimov_sig(s_hist, b_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimisation!\n",
    "def run(num_bins):\n",
    "    bins = jnp.linspace(0, 8, num_bins + 1)\n",
    "    loss = partial(asimov_pipe, s=sig, b=bkg, bandwidth=1e-1)\n",
    "    solver = OptaxSolver(loss, adam(learning_rate=1e-3), maxiter=10000, tol=1e-6)\n",
    "    opt = solver.run(bins).params\n",
    "    return opt\n",
    "\n",
    "\n",
    "# scan over a range of bin numbers to find the best significance\n",
    "bin_number_range_to_scan = [2, 4, 6, 8]  # add more bins here as you like\n",
    "results = [run(i) for i in bin_number_range_to_scan]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_bins(ax, bins_opt, i) -> None:\n",
    "    ax.hist(\n",
    "        [bkg, sig],\n",
    "        stacked=True,\n",
    "        bins=bins_opt,\n",
    "        histtype=\"step\",\n",
    "        label=[\"background\", \"signal\"],\n",
    "    )\n",
    "    ax.legend()\n",
    "    significance = hard_asimov_pipe(bins_opt, sig, bkg)\n",
    "    ax.set_title(f\"#bins={len(bins_opt)-1}, sig={significance:.3f}\")\n",
    "\n",
    "\n",
    "# plot helper function I wrote for automatic grids\n",
    "# see github.com/phinate/plothelp\n",
    "_ = autogrid(\n",
    "    data=results,\n",
    "    subplot_kwargs={\"sharex\": True, \"sharey\": True},\n",
    "    plot_func=plot_bins,\n",
    "    title=\"optimised bins (stat-only)\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Systematic-aware binning (prototype)\n",
    "\n",
    "The above code concerns the problem of optimising with known background. A more realistic scenario would be that we have some kind of uncertainty on this background, which would strongly affect the significance if it isn't negligible.\n",
    "\n",
    "Not to worry -- we can still optimise while accounting for this uncertainty! The issue is that we're now forced to describe in detail our statistical modelling step, as this will let us calculate the significance in a way that includes the treatment of nuisance parameters that model the background uncertainty.\n",
    "\n",
    "We'll look at the case of a simple three-point systematic, which contains a nominal estimation of the background histogram at the best estimate of some physical parameter, e.g. the jet energy scale (JES), but also two variations of the background that correspond to simulating physics at+-1 standard deviations away from that estimate. Here, we're just going to transpose the background data to the right and the left by a shift value.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shift = 0.4\n",
    "bup = bkg + shift\n",
    "bdown = bkg - shift\n",
    "\n",
    "# specify the workspace for a simple model with a three-point background systematic\n",
    "def correlated_background(signal, bkg, bkg_up, bkg_down):\n",
    "    spec = {\n",
    "        \"channels\": [\n",
    "            {\n",
    "                \"name\": \"single_channel\",\n",
    "                \"samples\": [\n",
    "                    {\n",
    "                        \"name\": \"signal\",\n",
    "                        \"data\": signal,\n",
    "                        \"modifiers\": [\n",
    "                            {\"name\": \"mu\", \"type\": \"normfactor\", \"data\": None}\n",
    "                        ],\n",
    "                    },\n",
    "                    {\n",
    "                        \"name\": \"background\",\n",
    "                        \"data\": bkg,\n",
    "                        \"modifiers\": [\n",
    "                            {\n",
    "                                \"name\": \"correlated_bkg_uncertainty\",\n",
    "                                \"type\": \"histosys\",\n",
    "                                \"data\": {\"hi_data\": bkg_up, \"lo_data\": bkg_down},\n",
    "                            }\n",
    "                        ],\n",
    "                    },\n",
    "                ],\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    return pyhf.Model(\n",
    "        spec, validate=False\n",
    "    )  # false validation enables the use of jax arrays\n",
    "\n",
    "\n",
    "# pipeline to calculate the full CLs for the model, given a number of bins\n",
    "def pipe(num_bins):\n",
    "    # make a dummy model with unit yields\n",
    "    model = correlated_background(*[jnp.ones(num_bins)] * 4)\n",
    "\n",
    "    def cls_pipe(bins, s, b, bup, bdown, bandwidth=1e-2):\n",
    "        # restrict bin edges so they don't overlap\n",
    "        bins_new = jnp.concatenate(\n",
    "            (\n",
    "                jnp.array([bins[0]]),\n",
    "                jnp.where(bins[1:] > bins[:-1], bins[1:], bins[:-1] + 1e-4),\n",
    "            ),\n",
    "            axis=0,\n",
    "        )\n",
    "        # make differentiable histograms for each dataset\n",
    "        s_hist = relaxed.hist(s, bins=bins_new, bandwidth=bandwidth) + 1e-4\n",
    "        b_hist = relaxed.hist(b, bins=bins_new, bandwidth=bandwidth) + 1e-4\n",
    "        bup_hist = relaxed.hist(bup, bins=bins_new, bandwidth=bandwidth) + 1e-4\n",
    "        bdown_hist = relaxed.hist(bdown, bins=bins_new, bandwidth=bandwidth) + 1e-4\n",
    "\n",
    "        # we hack our new values into the model since we already have the structure\n",
    "        # NB: this only approximates the model (there's other metadata to edit somewhere)\n",
    "        # but it's good enough for a loose optimisation, and it's super fast!\n",
    "        model.main_model.nominal_rates = jnp.stack([b_hist, s_hist]).reshape(\n",
    "            model.main_model.nominal_rates.shape\n",
    "        )\n",
    "        model.main_model.modifiers_appliers[\"histosys\"]._histosys_histoset = [\n",
    "            [[bdown_hist, b_hist, bup_hist], [s_hist] * 3]\n",
    "        ]\n",
    "\n",
    "        # differentiabe cls calculation\n",
    "        nominal = (\n",
    "            jnp.array(model.config.suggested_init()).at[model.config.poi_index].set(0.0)\n",
    "        )\n",
    "        data = model.expected_data(nominal)\n",
    "        pval = relaxed.infer.hypotest(\n",
    "            1, lr=1e-3, model=model, data=data, test_stat=\"q\", expected_pars=nominal\n",
    "        )\n",
    "        return pval\n",
    "\n",
    "    return cls_pipe\n",
    "\n",
    "\n",
    "# use true histograms to calculate the CLs for a given binning\n",
    "def pipe_uncert_hard(bins, s, b):\n",
    "    s_hist = jnp.histogram(s, bins=bins)[0] + 1e-4\n",
    "    b_hist = jnp.histogram(b, bins=bins)[0] + 1e-4\n",
    "    bup_hist = jnp.histogram(bup, bins=bins)[0] + 1e-4\n",
    "    bdown_hist = jnp.histogram(bdown, bins=bins)[0] + 1e-4\n",
    "    uncert = jnp.abs(bup_hist - bdown_hist) + 1e-4\n",
    "    model = correlated_background(s_hist, b_hist, bup_hist, bdown_hist)\n",
    "    nominal = model.config.suggested_init()\n",
    "    data = model.expected_data(nominal)\n",
    "    pval = pyhf.infer.hypotest(0, pdf=model, data=data, test_stat=\"q0\")\n",
    "    return jsp.stats.norm.ppf(1 - pval), [s_hist, b_hist, uncert]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimisation!\n",
    "def run_uncert(num_bins):\n",
    "    bins = jnp.linspace(0, 8, num_bins + 1)\n",
    "    cls_pipe = pipe(num_bins)\n",
    "    loss = partial(cls_pipe, s=sig, b=bkg, bup=bup, bdown=bdown, bandwidth=1e-1)\n",
    "    solver = OptaxSolver(loss, adam(learning_rate=1e-3), maxiter=10000, tol=1e-6)\n",
    "    opt = solver.run(bins).params\n",
    "    return opt  # , pipe_uncert_hard(opt, sig, bkg)\n",
    "\n",
    "\n",
    "# slower than before, since we're doing a full CLs calculation\n",
    "bin_number_range_to_scan = [4, 8]  # add more bins here as you like\n",
    "many = [run_uncert(i) for i in bin_number_range_to_scan]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_bins(ax, bins_opt, i) -> None:\n",
    "    ax.hist(\n",
    "        [bkg, sig],\n",
    "        stacked=True,\n",
    "        bins=bins_opt,\n",
    "        histtype=\"step\",\n",
    "        label=[\"background\", \"signal\"],\n",
    "    )\n",
    "    ax.hist(\n",
    "        bdown.tolist(),\n",
    "        bins=bins_opt,\n",
    "        histtype=\"step\",\n",
    "        label=\"bdown\",\n",
    "        alpha=0.5,\n",
    "        linestyle=\"--\",\n",
    "    )\n",
    "    ax.hist(\n",
    "        bup.tolist(),\n",
    "        bins=bins_opt,\n",
    "        histtype=\"step\",\n",
    "        label=\"bup\",\n",
    "        alpha=0.5,\n",
    "        linestyle=\"--\",\n",
    "    )\n",
    "    ax.legend()\n",
    "    significance, _ = pipe_uncert_hard(bins_opt, sig, bkg)\n",
    "    ax.set_title(f\"#bins={len(bins_opt)-1}, sig={significance:.3f}\")\n",
    "\n",
    "\n",
    "_ = autogrid(\n",
    "    data=many,\n",
    "    subplot_kwargs={\"sharex\": True, \"sharey\": True},\n",
    "    plot_func=plot_bins,\n",
    "    title=\"optimised bins (including systematic uncertainty)\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparison to stat-only:\n",
    "_ = autogrid(\n",
    "    data=results,\n",
    "    subplot_kwargs={\"sharex\": True, \"sharey\": True},\n",
    "    plot_func=plot_bins,\n",
    "    title=\"optimised bins\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "22d6333b89854cd01c2018f3ca2f5a59a2cde2765fbca789ff36cfad48ca629b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
