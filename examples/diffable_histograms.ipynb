{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cef825ee",
   "metadata": {},
   "source": [
    "# Differentiable Histograms\n",
    "\n",
    "One of the most common data structures in HEP is the **histogram**.\n",
    "\n",
    "It is a very powerful data reduction and density estimation technique.\n",
    "\n",
    "Unfortunately it is not differentiable :(. However, through the use\n",
    "of \"soft relaxations\" we can achieve *differentiable histograms*.\n",
    "\n",
    "Check out this preprint https://arxiv.org/abs/2203.05570 for more information, but at the core these\n",
    "are binned Kernel Density Estimates (bKDE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5316505",
   "metadata": {},
   "outputs": [],
   "source": [
    "import relaxed\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d511200d",
   "metadata": {},
   "source": [
    "The trick is that instead of using a \"hard\" histogram function like `np.histogram` (`jnp.histogram` doesn't mean it's differentiable, but for fun you can try what happens if you use that) you would use a \"relaxed\" version from the `relaxed` library.\n",
    "\n",
    "As a user you will need to specify the degree of relaxation and this is given by the \"bandwidth\" of the kernel density estimate. Try playing with the bandwidth parameter to see the effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ecf1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = np.random.normal(1, 1, size=1000)\n",
    "bins = np.linspace(-10, 10, 51)\n",
    "ctrs = bins[:-1] + np.diff(bins) / 2\n",
    "counts_soft_p1 = relaxed.hist(inputs, bandwidth=0.5, bins=bins)\n",
    "counts_soft_p2 = relaxed.hist(inputs, bandwidth=1.0, bins=bins)\n",
    "\n",
    "counts_hard, _ = np.histogram(inputs, bins=bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fde4ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.step(ctrs, counts_hard, label=\"hard\", where=\"mid\")\n",
    "plt.step(ctrs, counts_soft_p1, label=\"small bw\", where=\"mid\")\n",
    "plt.step(ctrs, counts_soft_p2, label=\"large bw\", where=\"mid\")\n",
    "plt.legend()\n",
    "plt.xlim(-5, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de316a2",
   "metadata": {},
   "source": [
    "# Optimizing through histogramming\n",
    "\n",
    "\n",
    "The main advantage if \"soft\" histograms is that you can optimize (or take gradients) throughout the histogramming operation. We will demonstrate this by doing a \"gradient-based\" shift of the histogram.\n",
    "\n",
    "I.e. if you have many events $x_i$ what is the value $\\Delta$ such that the peak of the histogram of  $x_i + \\Delta$ is at some target value (e.g. t=3.0)\n",
    "\n",
    "For this we will set up a loss term \n",
    "\n",
    "$$L(\\Delta) = (\\mathrm{mean}(\\mathrm{histo}(x_i+\\Delta)) - t)^2$$\n",
    "\n",
    "and we will the minimize $L$ by computing gradients \n",
    "\n",
    "$$\\partial L \\over \\partial \\Delta$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "981cdb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shift_histo(factor, inputs):\n",
    "    shifted_inputs = factor + inputs\n",
    "    #     soft_counts,_ = jax.numpy.histogram(shifted_inputs, bins = bins)\n",
    "    soft_counts = relaxed.hist(shifted_inputs, bandwidth=0.5, bins=bins)\n",
    "    binned_mean = np.sum(ctrs * soft_counts / len(shifted_inputs))\n",
    "    return binned_mean\n",
    "\n",
    "\n",
    "def loss(param, target, inputs):\n",
    "    binned_mean = shift_histo(param, inputs)\n",
    "    loss = (binned_mean - target) ** 2\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498c14c3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import jax\n",
    "import optax\n",
    "\n",
    "opt = optax.adam(4e-2)\n",
    "params = 0.0\n",
    "state = opt.init(params)\n",
    "target = 3.0\n",
    "\n",
    "for i in range(100):\n",
    "    lvalue, g = jax.value_and_grad(loss)(params, target, inputs)\n",
    "    updates, state = opt.update(g, state)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    if i % 10 == 0:\n",
    "        print(params, lvalue)\n",
    "\n",
    "        hard_hist = np.histogram(params + inputs, bins=bins)[0]\n",
    "        hard_mean = np.sum(ctrs * hard_hist / len(inputs))\n",
    "        plt.step(ctrs, hard_hist, where=\"mid\", color=\"maroon\")\n",
    "        plt.xlim(-5, 5)\n",
    "        plt.ylim(0, 200)\n",
    "        plt.vlines(hard_mean, 0, 200, colors=\"maroon\")\n",
    "        plt.vlines(target, 0, 200, colors=\"k\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65ffb2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
